# An Open-Source AI Platform for Augmented Talks and Interactive Real-Time Presentations
- whitepaper -

We are now in an AI era where retrieval-augmented generation (RAG) is a game-changer and a facilitator for technological leaps. One of the opportunities I see in this regard is the concept of Augmented Talk.

At the beginning of 2024, I decided to elevate the way I conduct live presentations to the next level, which led me to work on this personal project, now considered a startup. I publicly used this platform at three events in early 2024:

- Electron by LSE, March 9th - Hackathon organized within the Faculty of ETTI
- InfoShare Gdansk, May 23rd - The largest tech & startup event in EEST
- DevTalks Bucharest, May 30th - The largest dev event in Romania

These initial events marked the beginning of Oratify. The purpose of these presentations was to introduce Oratify by using it and to debut the concept of Augmented Talk.

## What Augmented Talk Means
- Typically, the speaker is on stage, delivering a monologue or presenting a pre-prepared topic.
- Sometimes, the topics may not fully engage the audience.
- In many cases, the speaker cannot obtain proper feedback from a large audience, especially in online sessions.
- Before the rise of GenAI, we had good tools for NLP and NLU, but generating answers was more challenging. Now, real-time RAG systems can leverage LLMs and strong software architecture to process data in real-time and generate complex answers.
- The core idea of an Augmented Talk is that the speaker can constantly use a software tool to gauge the audience's opinions on the current topic and steer the discussion based on their preferences.
- Additionally, the speaker can preprocess their script, allowing users to answer in real-time while the speaker discusses specific slides.
- Displaying everyone's answers on a common screen allows the audience to see collective thoughts and votes.
- The software platform can provide each audience participant with customized answers based on the speaker’s script or live answers from others.
- It can also process all interactions, generating summaries, conclusions, or extracting main topics simultaneously.
- Involving multimodal transformer NNs in the communication flow can eliminate language barriers and accessibility issues.
- The system can use multimodal NNs to interchangeably transform text, audio, image, and video.
- The system can leverage LLMs to correct misspellings, translate messages across multiple languages, and augment or rephrase ideas.

## Summary of Core Concepts of Augmented Talks
- Speakers can receive real-time feedback, opinions, questions, and votes from the audience.
- The audience can get real-time tailored answers from the speaker and others.
- Everyone in the group can better understand each other’s interests and opinions.
- The discussion path is dynamically generated during the session, enhancing communication efficiency to unprecedented levels.
- It eliminates language barriers, allowing everyone to speak in their native languages.
- It provides better accessibility across text, video, image, and audio channels.
- It creates summaries, and conclusions, and extracts main ideas.

## Oratify is an open source platform that implements this concept. Key Features
- Speakers can prepare presentations or embed existing slides such as Google Slides or PDF pages.
- Speakers can load notes, scripts, and other ideas into the application as plain text.
- Speakers can prepare questions in different formats (text, multiple choice, etc.).
- Speakers can drive the presentation flow.
- Participants can vote or answer speaker questions in real-time and receive real-time answers from the speaker or others in the audience.
- Everyone can see the results of voting or answers on the speaker's view.
- Everyone can receive conclusions and follow-ups after the talk.
- Everyone can view attendance stats and the talk duration or remaining time.
- Participants can register and share their data or remain anonymous.
